<!DOCTYPE html>
<html>
<head>    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>    
      Research Overview &ndash;
    
    Dibya Ghosh</title>
    <meta name="description" content="Dibya Ghosh's Personal Website'">

    <link type="text/css" rel="stylesheet" href="/css/main.css">
    <script src="https://code.jquery.com/jquery-3.1.1.slim.min.js" integrity="sha384-A7FZj7v+d/sdmMqp/nOQwliLvUsJfDHW+k9Omg/a/EheAdgtzNs3hpfag6Ed950n" crossorigin="anonymous"></script>
    <link type="text/css" rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <link type="text/css" rel="stylesheet" href="/css/fira_font.css">

 <link rel="shortcut icon" href="/favicon.ico">

<script src="https://cdnjs.cloudflare.com/ajax/libs/slideout/1.0.1/slideout.min.js"></script>
<script type="text/javascript" src="/js/menu.js"></script>
   


</head>


<body>

    <nav id="menu" class="menu">
      <header>
        <section class="menu-section">
          <h3 class="menu-section-title">Menu</h3>
          <ul class="menu-section-list">
              <li><a class="page-link" href="/">Home</a>
              <li><a class="page-link" href="https://dibyaghosh.com/blog">Blog</a></li>
             <li><a class="page-link" href="/research">Research</a></li>
              <li><a class="page-link" href="/about">About</a></li>
          </ul>
        </section>
      </header>
    </nav>
    

    <main id="panel">
      <div class="page-content">
        <div class="wrapper container">
          <header>
            <nav class="site-header">
              <div class="row">
                <div class="menu-toggle"><button class="toggle-button"><i class="icon fa fa-bars"></i></button></div>
              <!-- <div class="icon-wrapper left"><a href="/blog"><i class="fa fa-rss"></i></a></div> -->

                <div class="site-title"><a href="/">Dibya Ghosh</a></div>
                <!-- <div class="icon-wrapper right"> <a href="http://labs.dibya.xyz"><i class="icon fa fa-flask"></i></a> </div> -->

              </div>
              <div class="desktop-menu">
                <ul>
                    <li><a href="/"> Home </a></li>
                    <li><a href="/research"> Research</a></li>
                    <li><a href="https://dibyaghosh.com/blog">Blog</a></li>
                    <li><a href="/about">About</a></li>
                </ul>
              </div>
            </nav>
          </header>


          <h1 class="page-heading">Research</h1>
<center>  Here's my <a href="https://scholar.google.com/citations?user=znnl0kwAAAAJ&hl=en&oi=sra">Google Scholar</a>. My <a onclick='alert("Dibya Ghosh -> Ani Adhikari -> Persi Diaconis -> Paul Erdos")'>Erdős number</a> is 3. My Bacon number is lamentably still undefined.  </center>

<div class="divider"></div>

<div class="row">
  
  
    <div class="three columns">
  <img style="margin-top:3em" src="/images/research/arc.png">
    </div>
    
    <div class="nine columns">
      
      <b><a href="https://arxiv.org/abs/1811.07819"> Learning Actionable Representations with Goal-Conditioned Policies</a></b>
      <p> <i> Dibya Ghosh</i>, Abhishek Gupta, Sergey Levine <br/>
      NeurIPS Deep RL Workshop 2018
    </p>
      <table>
        <tr>
          <td><a href="https://arxiv.org/abs/1811.07819">ArXiv</a></td>
          <td><a href="https://sites.google.com/view/arc-reps">Website</a></td>
          <td></td>
        </tr>
      </table>
      <div class="four columns"></div>
      <div class="four columns"></div>
      <div class="three columns"></div>
  <br />   
   <p>In this paper, we present ARCs, a representation learning algorithm which attempts to optimize <i> functionally relevant </i> elements of state. ARCs relate distance between states in latent space with the actions required to reach the state, which implicitly captures system dynamics and ignores uncontrollable factors. ARCs are useful for exploration, features for policies, and for developing hierarchies.
      </p>
    </div>
  
  
    </div>
    
      <div class="divider"></div>

<div class="row">
  
  
    <div class="three columns">
  <img style="margin-top:3em" src="/images/research/vice.png">
    </div>
    
    <div class="nine columns">
      
      <b><a href="https://sites.google.com/view/inverse-event"> Variational Inverse Control with Events</a></b>
      <p> Justin Fu*, Avi Singh*, <i> Dibya Ghosh</i>, Larry Yang, Sergey Levine <br/>
      NeurIPS 2018
    </p>
      <table>
        <tr>
          <td><a href="https://arxiv.org/abs/1805.11686">ArXiv</a></td>
          <td><a href="https://sites.google.com/view/inverse-event">Website</a></td>
          <td></td>
        </tr>
      </table>
      <div class="four columns"></div>
      <div class="four columns"></div>
      <div class="three columns"></div>
  <br />   
   <p>In this paper, we present VICE, a method which generalizes inverse optimal control to learning reward functions from goal examples. By requiring only goal examples, and not demonstration
     trajectories, VICE is well suited for real-world robotics tasks, in which reward specification is difficult.
      </p>
    </div>
  
  
    </div>
    
      <div class="divider"></div>

  
<div class="row">
  
  
  <div class="three columns">
<img style="margin-top:3em" src="/images/research/dnc.png">
  </div>
  
  <div class="nine columns">
    
    <b><a href="http://dibyaghosh.com/dnc/">Divide-and-Conquer Reinforcement Learning</a></b>
    <p><i> Dibya Ghosh</i>, Avi Singh, Aravind Rajeswaran, Vikash Kumar, Sergey Levine <br/>
    ICLR 2018.
  </p>
    <table>
      <tr>
        <td><a href="https://arxiv.org/abs/1711.09874">ArXiv</a></td>
        <td><a href="http://dibyaghosh.com/dnc/">Website</a></td>
        <td><a href="https://github.com/dibyaghosh/dnc">Code</a></td>
      </tr>
    </table>
    <div class="four columns"></div>
    <div class="four columns"></div>
    <div class="three columns"></div>
<br />   
 <p>In this paper, we present a method for scaling model-free deep reinforcement learning methods to tasks with high stochasticity
      in initial state and task distributions. We demonstrate our method on a suite of challenging sparse-reward manipulation tasks that were unsolved by prior work.
    </p>
  </div>


  </div>
  
  
</div>
<div class="divider"></div>



        </div>

      </div>

      <div class="invisible-divider"></div>
      <footer class="site-footer">
  <div class="container">
    <div class="row">
      <div class="force six columns">
        <ul class="contact-list">
          <!-- <li>Dibya Ghosh</li> -->
          <li>dibya (at) berkeley (dot) edu</li>
          <li class="social-links">
            <a href="https://twitter.com/its_dibya" target="_blank">
              <i class="fa fa-twitter"></i>
          </a>
            <a href="https://www.linkedin.com/in/dibyaghosh" target="_blank">
                <i class="fa fa-linkedin"></i>
            </a>
            <a href="https://github.com/dibyaghosh/" target="_blank">
                <i class="fa fa-github"></i>
            </a>
          </li>
        </ul>
      </div>
      <div class="force six columns">
        <p class="site-description">
            Site designed by Dibya Ghosh<br/>
            <!-- Modified from <a href='https://olivierpieters.be/'>Olivier Pieters</a> and <a href='https://distill.pub/'>Distill</a>. <br /> -->
            Built with <i> Jekyll. 
            <a rel="license" href="/about/#license">©&nbsp;2017</a>
        </p>
      </div>
    </div>
  </div>
</footer>

    </main>

  </body>

</html>
